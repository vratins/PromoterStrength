{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00a681d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d06cf11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total memory: 93.06243133544922 GB\n",
      "Number of CPUs: 64\n",
      "PyTorch is using 64 threads.\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "import torch\n",
    "import os\n",
    "import multiprocessing\n",
    "\n",
    "print('Total memory:', psutil.virtual_memory().total / (1024 ** 3), \"GB\")\n",
    "print(\"Number of CPUs:\", multiprocessing.cpu_count())\n",
    "torch.set_num_threads(multiprocessing.cpu_count())\n",
    "\n",
    "print(\"PyTorch is using\", torch.get_num_threads(), \"threads.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdb35692",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('filtered_data.csv')\n",
    "sequences = list(data['seq'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08a1582b",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = max(len(seq) for seq in sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9361187",
   "metadata": {},
   "source": [
    "## Embedding Generation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c4f7505-5b72-4697-85c8-dc1cfd2105b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sequences: 100%|██████████| 850/850 [02:58<00:00,  4.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Embeddings shape: torch.Size([54383, 512])\n"
     ]
    }
   ],
   "source": [
    "#if you're running on a GPU:\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"InstaDeepAI/nucleotide-transformer-v2-50m-multi-species\", trust_remote_code=True)\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"InstaDeepAI/nucleotide-transformer-v2-50m-multi-species\", trust_remote_code=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "sequences_short = sequences  \n",
    "batch_size = 64 \n",
    "batches = [sequences_short[i:i + batch_size] for i in range(0, len(sequences_short), batch_size)]\n",
    "\n",
    "all_embeddings = []\n",
    "\n",
    "with torch.no_grad(): \n",
    "    for batch in tqdm(batches, desc=\"Processing sequences\"):\n",
    "        tokens_ids = tokenizer.batch_encode_plus(batch, return_tensors=\"pt\", padding=\"max_length\", max_length=max_length)[\"input_ids\"]\n",
    "        attention_mask = tokens_ids != tokenizer.pad_token_id\n",
    "\n",
    "        #move tensors to GPU\n",
    "        tokens_ids = tokens_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "\n",
    "        #model forward pass\n",
    "        torch_outs = model(\n",
    "            tokens_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            encoder_attention_mask=attention_mask,\n",
    "            output_hidden_states=True\n",
    "        )\n",
    "\n",
    "        embeddings = torch_outs['hidden_states'][-1]\n",
    "        attention_mask = torch.unsqueeze(attention_mask, dim=-1)\n",
    "        mean_sequence_embeddings = torch.sum(attention_mask * embeddings, dim=-2) / torch.sum(attention_mask, dim=1)\n",
    "\n",
    "        all_embeddings.append(mean_sequence_embeddings)\n",
    "\n",
    "#move the final result to CPU and convert to NumPy array\n",
    "all_embeddings = torch.cat(all_embeddings, dim=0).cpu()\n",
    "\n",
    "print(f\"All Embeddings shape: {all_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ee51751-f1f1-484b-8b0b-c74a1bb34a5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111376790"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#saving to a npy file:\n",
    "import pickle\n",
    "\n",
    "np.save('all_embeddings.npy', all_embeddings)\n",
    "\n",
    "with open('embeddings.pkl', 'wb') as f:\n",
    "    pickle.dump(all_embeddings, f)\n",
    "\n",
    "pickle_file_size = os.path.getsize('embeddings.pkl')\n",
    "\n",
    "pickle_file_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230e93d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CPU Implementation\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"InstaDeepAI/nucleotide-transformer-v2-50m-multi-species\", trust_remote_code=True)\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"InstaDeepAI/nucleotide-transformer-v2-50m-multi-species\", trust_remote_code=True)\n",
    "\n",
    "sequences_short = sequences\n",
    "batch_size = 48 \n",
    "batches = [sequences_short[i:i + batch_size] for i in range(0, len(sequences_short), batch_size)]\n",
    "\n",
    "all_embeddings = []\n",
    "\n",
    "for batch in tqdm(batches, desc=\"Processing sequences\"):\n",
    "    tokens_ids = tokenizer.batch_encode_plus(batch, return_tensors=\"pt\", padding=\"max_length\", max_length=max_length)[\"input_ids\"]\n",
    "    attention_mask = tokens_ids != tokenizer.pad_token_id\n",
    "\n",
    "    torch_outs = model(\n",
    "        tokens_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        encoder_attention_mask=attention_mask,\n",
    "        output_hidden_states=True\n",
    "    )\n",
    "\n",
    "    embeddings = torch_outs['hidden_states'][-1].detach().numpy()\n",
    "    attention_mask = torch.unsqueeze(attention_mask, dim=-1)\n",
    "    mean_sequence_embeddings = torch.sum(attention_mask * embeddings, axis=-2) / torch.sum(attention_mask, axis=1)\n",
    "\n",
    "    all_embeddings.append(mean_sequence_embeddings)\n",
    "\n",
    "all_embeddings = torch.cat(all_embeddings, dim=0)\n",
    "\n",
    "print(f\"All Embeddings shape: {all_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99554c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc83b4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
